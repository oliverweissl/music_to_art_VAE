{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f1a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf; tf.compat.v1.disable_eager_execution()\n",
    "import tensorflow.compat.v1 as tfc\n",
    "\n",
    "from csv import reader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import audio_data_gen as adg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4999beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "gpu_options= tfc.GPUOptions(per_process_gpu_memory_fraction = 0.8)\n",
    "\n",
    "try: sess.close() \n",
    "except: pass\n",
    "sess = tfc.InteractiveSession(config=tfc.ConfigProto(gpu_options=gpu_options))\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a459391",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = \"adj_pic\"\n",
    "MAX_SIZE = (962,962)\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9dfcc",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf21e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND = random.randint(0, 100)\n",
    "@tf.function\n",
    "def img_to_arr(file_name):\n",
    "    image = tf.io.read_file(file_name)\n",
    "    image = tf.io.decode_jpeg(image,channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float16)/255\n",
    "    s = tf.shape(image)[1]\n",
    "    image = tf.reshape(image, [s,MAX_SIZE[0],3])\n",
    "    output = image[RAND]\n",
    "    return output,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ccd8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(f\"{LIB}/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57228f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.list_files(file_list).map(img_to_arr,num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "ds = ds.take(32000)\n",
    "ds = ds.batch(batch_size= BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11620985",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742cf269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Conv1D, Conv1DTranspose, Flatten, Lambda, Reshape\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c479f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = MAX_SIZE[0]\n",
    "num_channels = 3\n",
    "latent_dim = 1443\n",
    "def compute_latent(x):\n",
    "    mu, sigma = x\n",
    "    batch = K.shape(mu)[0]\n",
    "    dim = K.int_shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch,dim))\n",
    "    return mu + K.exp(sigma/2)*eps\n",
    "\n",
    "def kl_reconstruction_loss(true, pred):\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_height\n",
    "    # KL divergence loss\n",
    "    kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    # Total loss = 50% rec + 50% KL divergence loss\n",
    "    return K.mean(reconstruction_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e12991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(img_height,num_channels))\n",
    "\n",
    "encoder_conv = Conv1D(filters=8, kernel_size=3, strides=2, \n",
    "                padding='same', activation='relu')(encoder_input)\n",
    "encoder_conv = Conv1D(filters=16, kernel_size=3, strides=2, \n",
    "                padding='same', activation='relu')(encoder_input)\n",
    "encoder = Flatten()(encoder_conv)\n",
    "\n",
    "mu = Dense(latent_dim)(encoder)\n",
    "sigma = Dense(latent_dim)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4ce4246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 481, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_space = Lambda(compute_latent, output_shape=(latent_dim,))([mu, sigma])\n",
    "conv_shape = K.int_shape(encoder_conv)\n",
    "conv_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "314292ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim,))\n",
    "decoder = Dense(conv_shape[1]*conv_shape[2], activation='relu')(decoder_input)\n",
    "decoder = Reshape((conv_shape[1], conv_shape[2]))(decoder)\n",
    "decoder_conv = Conv1DTranspose(filters=16, kernel_size=3, strides=2, \n",
    "                           padding='same', activation='relu')(decoder)\n",
    "decoder_conv = Conv1DTranspose(filters=8, kernel_size=3, strides=2, \n",
    "                           padding='same', activation='relu')(decoder)\n",
    "decoder_conv =  Conv1DTranspose(filters=num_channels, kernel_size=3, \n",
    "                          padding='same', activation='sigmoid')(decoder_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d67a4adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 962, 3)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 481, 16)      160         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 7696)         0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1443)         11106771    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1443)         11106771    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1443)         0           ['dense[0][0]',                  \n",
      "                                                                  'dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,213,702\n",
      "Trainable params: 22,213,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1443)]            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7696)              11113024  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 481, 16)           0         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 962, 8)           392       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv1d_transpose_2 (Conv1DT  (None, 962, 3)           75        \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,113,491\n",
      "Trainable params: 11,113,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Model(encoder_input, latent_space)\n",
    "encoder.summary()\n",
    "decoder = Model(decoder_input, decoder_conv)\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63bd2193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 962, 3)]          0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 1443)              22213702  \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 962, 3)            11113491  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,327,193\n",
      "Trainable params: 33,327,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae = Model(encoder_input, decoder(encoder(encoder_input)))\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54a55b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='adam', loss=kl_reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a698aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    776\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:616\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    596\u001b[0m         model,\n\u001b[0;32m    597\u001b[0m         x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m         validation_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    613\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size,\n\u001b[0;32m    614\u001b[0m                                                    steps_per_epoch, x)\n\u001b[1;32m--> 616\u001b[0m   x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m      \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m validation_data:\n\u001b[0;32m    629\u001b[0m     val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_prepare_validation_data(\n\u001b[0;32m    630\u001b[0m         validation_data, batch_size, validation_steps)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2282\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2279\u001b[0m training_utils_v1\u001b[38;5;241m.\u001b[39mvalidate_dataset_input(x, y, sample_weight,\n\u001b[0;32m   2280\u001b[0m                                          validation_split)\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m-> 2282\u001b[0m   \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_dataset_shuffled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2284\u001b[0m is_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_tensors_from_dataset:\n\u001b[0;32m   2286\u001b[0m   \u001b[38;5;66;03m# We do this for `train_on_batch`/etc.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:1556\u001b[0m, in \u001b[0;36mverify_dataset_shuffled\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1547\u001b[0m \u001b[38;5;124;03m\"\"\"Verifies that the dataset is shuffled.\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m \n\u001b[0;32m   1549\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;124;03m  boolean, whether the input dataset is shuffled or not.\u001b[39;00m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)\n\u001b[1;32m-> 1556\u001b[0m graph_def \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_graph_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph_def\u001b[38;5;241m.\u001b[39mnode:\n\u001b[0;32m   1558\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShuffleDataset\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:1542\u001b[0m, in \u001b[0;36mget_dataset_graph_def\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m   1540\u001b[0m   graph_def_str \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39m_as_serialized_graph()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1542\u001b[0m   graph_def_str \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_serialized_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mGraphDef()\u001b[38;5;241m.\u001b[39mFromString(graph_def_str)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py:3940\u001b[0m, in \u001b[0;36mget_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   3937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   3939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m x\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m-> 3940\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1060\u001b[0m, in \u001b[0;36mTensor.eval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, feed_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1037\u001b[0m   \u001b[38;5;124;03m\"\"\"Evaluates this tensor in a `Session`.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m  Note: If you are not using `compat.v1` libraries, you should not need this,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;124;03m    A numpy array corresponding to the value of this tensor.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1060\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_eval_using_default_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5769\u001b[0m, in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5765\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m session\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m graph:\n\u001b[0;32m   5766\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use the given session to evaluate tensor: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5767\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms graph is different from the session\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5768\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 5769\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    964\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 967\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    970\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1190\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1190\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1193\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1370\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1370\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1377\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1376\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1379\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1360\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1358\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1360\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1453\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1452\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1453\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = vae.fit(ds, epochs=5, validation_data=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "171b3ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x267942ab220>]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ebce7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: vae\\assets\n",
      "INFO:tensorflow:Assets written to: dec\\assets\n",
      "INFO:tensorflow:Assets written to: enc\\assets\n"
     ]
    }
   ],
   "source": [
    "vae.save('vae')\n",
    "decoder.save('dec')\n",
    "encoder.save('enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca5552",
   "metadata": {},
   "source": [
    "### Gen new image from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a947b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Oliver\\Music_to_Art_GAN\\audio_data_gen.py:28: FutureWarning: Pass y=[0.         0.         0.         ... 0.0021078  0.00191791 0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  mfccs = librosa.feature.mfcc(x, sr=sr)\n",
      "D:\\Oliver\\Music_to_Art_GAN\\audio_data_gen.py:29: FutureWarning: Pass y=[0.         0.         0.         ... 0.0021078  0.00191791 0.        ] as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  chromagram = librosa.feature.chroma_stft(x, sr=sr, hop_length=hop_length)\n"
     ]
    }
   ],
   "source": [
    "decoder = tf.keras.models.load_model('dec')\n",
    "decoder.compile(optimizer='adam', loss=kl_reconstruction_loss)\n",
    "\n",
    "audio = \"motive.wav\"\n",
    "name = audio.split(\".\")[0]\n",
    "adg.extract(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da994e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]C:\\Users\\Oliver\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "5310it [00:32, 162.60it/s]\n"
     ]
    }
   ],
   "source": [
    "FILE = glob.glob(f\"{audio}_*_1443.csv\")[0]\n",
    "with open(FILE) as csv_file:\n",
    "    csv_reader = reader(csv_file)\n",
    "    head = next(csv_reader)\n",
    "    \n",
    "    arr = []\n",
    "    for row in tqdm(csv_reader):\n",
    "        row = np.resize(np.expand_dims(np.array(row),axis = 0),(1,1443))\n",
    "\n",
    "        pred_im = decoder.predict((row,))\n",
    "        arr.append(pred_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "38620e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_n = np.squeeze(np.array(arr)*255,axis =1)\n",
    "img = Image.fromarray(arr_n , 'RGB')\n",
    "img.save(f\"{name}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8b75b6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a33e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
